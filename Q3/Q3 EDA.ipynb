{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "In this part we will try to get insights in the data. We will also visualize it to see different relationships between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploratory data analysis (EDA)\n",
    "\n",
    "df = pd.read_csv('../datasets/Data_Q3/D21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.151624</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.132203</td>\n",
       "      <td>0.296852</td>\n",
       "      <td>-0.068419</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>0.357298</td>\n",
       "      <td>0.150411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.210474</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.018591</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.122228</td>\n",
       "      <td>0.090209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>-0.184451</td>\n",
       "      <td>-0.051672</td>\n",
       "      <td>0.009168</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.124602</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>0.046270</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         1         2         3         4         5         6  7\n",
       "0           0  0.046303  0.028466  0.151624  0.007990  0.005973  0.003635  1\n",
       "1           1  0.132203  0.296852 -0.068419  0.146222  0.357298  0.150411  0\n",
       "2           2 -0.210474  0.144329  0.018591  0.097309  0.122228  0.090209  1\n",
       "3           3  0.007505 -0.184451 -0.051672  0.009168  0.023768  0.011703  1\n",
       "4           4 -0.081010  0.124602 -0.033422  0.046270  0.032020  0.008429  1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few rows of original data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0', '1', '2', '3', '4', '5', '6', '7']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.151624</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.132203</td>\n",
       "      <td>0.296852</td>\n",
       "      <td>-0.068419</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>0.357298</td>\n",
       "      <td>0.150411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.210474</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.018591</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.122228</td>\n",
       "      <td>0.090209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007505</td>\n",
       "      <td>-0.184451</td>\n",
       "      <td>-0.051672</td>\n",
       "      <td>0.009168</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.124602</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>0.046270</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6  7\n",
       "0  0.046303  0.028466  0.151624  0.007990  0.005973  0.003635  1\n",
       "1  0.132203  0.296852 -0.068419  0.146222  0.357298  0.150411  0\n",
       "2 -0.210474  0.144329  0.018591  0.097309  0.122228  0.090209  1\n",
       "3  0.007505 -0.184451 -0.051672  0.009168  0.023768  0.011703  1\n",
       "4 -0.081010  0.124602 -0.033422  0.046270  0.032020  0.008429  1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deleting the first column since it is use less numbering only\n",
    "\n",
    "my_df = df.drop('Unnamed: 0', axis = 1)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1802\n",
       "0     452\n",
       "Name: 7, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting the number of lables ( it is an unbalanced dataset )\n",
    "my_df['7'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2254 entries, 0 to 2253\n",
      "Data columns (total 7 columns):\n",
      "1    2254 non-null float64\n",
      "2    2254 non-null float64\n",
      "3    2254 non-null float64\n",
      "4    2254 non-null float64\n",
      "5    2254 non-null float64\n",
      "6    2254 non-null float64\n",
      "7    2254 non-null int64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 123.3 KB\n"
     ]
    }
   ],
   "source": [
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "      <td>2254.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.055471</td>\n",
       "      <td>0.097492</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.052747</td>\n",
       "      <td>0.799468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.145812</td>\n",
       "      <td>0.174923</td>\n",
       "      <td>0.121642</td>\n",
       "      <td>0.064531</td>\n",
       "      <td>0.076833</td>\n",
       "      <td>0.071523</td>\n",
       "      <td>0.400488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.458801</td>\n",
       "      <td>-0.717313</td>\n",
       "      <td>-0.607233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.037985</td>\n",
       "      <td>-0.008967</td>\n",
       "      <td>-0.051737</td>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.062039</td>\n",
       "      <td>0.107267</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>0.022632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.134210</td>\n",
       "      <td>0.225356</td>\n",
       "      <td>0.078702</td>\n",
       "      <td>0.053722</td>\n",
       "      <td>0.062584</td>\n",
       "      <td>0.070451</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.774888</td>\n",
       "      <td>0.701873</td>\n",
       "      <td>0.656722</td>\n",
       "      <td>0.676036</td>\n",
       "      <td>0.645487</td>\n",
       "      <td>0.691162</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1            2            3            4            5  \\\n",
       "count  2254.000000  2254.000000  2254.000000  2254.000000  2254.000000   \n",
       "mean      0.055471     0.097492     0.015423     0.047272     0.052910   \n",
       "std       0.145812     0.174923     0.121642     0.064531     0.076833   \n",
       "min      -0.458801    -0.717313    -0.607233     0.000000     0.000000   \n",
       "25%      -0.037985    -0.008967    -0.051737     0.011973     0.008963   \n",
       "50%       0.062039     0.107267     0.010764     0.023644     0.022970   \n",
       "75%       0.134210     0.225356     0.078702     0.053722     0.062584   \n",
       "max       0.774888     0.701873     0.656722     0.676036     0.645487   \n",
       "\n",
       "                 6            7  \n",
       "count  2254.000000  2254.000000  \n",
       "mean      0.052747     0.799468  \n",
       "std       0.071523     0.400488  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.008504     1.000000  \n",
       "50%       0.022632     1.000000  \n",
       "75%       0.070451     1.000000  \n",
       "max       0.691162     1.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI4AAANeCAYAAAB08kU4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f3a96b9860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting histograms of the features and also label\n",
    "my_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_matrix = my_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    1.000000\n",
       "2    0.072545\n",
       "3    0.004242\n",
       "1   -0.117944\n",
       "6   -0.279816\n",
       "4   -0.398477\n",
       "5   -0.491877\n",
       "Name: 7, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix['7'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.213905</td>\n",
       "      <td>0.055283</td>\n",
       "      <td>-0.040040</td>\n",
       "      <td>0.017195</td>\n",
       "      <td>-0.081457</td>\n",
       "      <td>-0.117944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.213905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045076</td>\n",
       "      <td>-0.054145</td>\n",
       "      <td>-0.079767</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.072545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.055283</td>\n",
       "      <td>-0.045076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017537</td>\n",
       "      <td>-0.069442</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.004242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.040040</td>\n",
       "      <td>-0.054145</td>\n",
       "      <td>-0.017537</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663467</td>\n",
       "      <td>0.575025</td>\n",
       "      <td>-0.398477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.017195</td>\n",
       "      <td>-0.079767</td>\n",
       "      <td>-0.069442</td>\n",
       "      <td>0.663467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>-0.491877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.081457</td>\n",
       "      <td>0.112730</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>0.575025</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.279816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.117944</td>\n",
       "      <td>0.072545</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.398477</td>\n",
       "      <td>-0.491877</td>\n",
       "      <td>-0.279816</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7\n",
       "1  1.000000 -0.213905  0.055283 -0.040040  0.017195 -0.081457 -0.117944\n",
       "2 -0.213905  1.000000 -0.045076 -0.054145 -0.079767  0.112730  0.072545\n",
       "3  0.055283 -0.045076  1.000000 -0.017537 -0.069442  0.010132  0.004242\n",
       "4 -0.040040 -0.054145 -0.017537  1.000000  0.663467  0.575025 -0.398477\n",
       "5  0.017195 -0.079767 -0.069442  0.663467  1.000000  0.589606 -0.491877\n",
       "6 -0.081457  0.112730  0.010132  0.575025  0.589606  1.000000 -0.279816\n",
       "7 -0.117944  0.072545  0.004242 -0.398477 -0.491877 -0.279816  1.000000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we observe that feature 3 is not correlated to any of the features so we can remove it if we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no null values in our data set we are now finished with the EDA\n",
    "\n",
    "Now we can move on to the machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dividing data to features and labels\n",
    "data = np.array(my_df)\n",
    "X = data[:,:6]\n",
    "Y = data[:,6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting data into training and test\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First doing logistic regression using scikit learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression( C = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time : 0.003007173538208008 seconds \n",
      "Accuracy using scikit learn Logistic Regression : 84.9223946785\n",
      "Precision : 0.838804327893\n",
      "Recall : 0.849223946785\n",
      "F1_score : 0.824165539152\n",
      "Roc_Auc_score : 0.652376033058\n"
     ]
    }
   ],
   "source": [
    "print('Execution time : ' + str(end_time - start_time) + ' seconds ')\n",
    "print('Accuracy using scikit learn Logistic Regression : ' + str(100 * accuracy_score(y_test, y_pred) ) )\n",
    "print('Precision : ' + str(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Recall : ' + str(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('F1_score : ' + str(f1_score(y_test, y_pred, average='weighted')))\n",
    "print('Roc_Auc_score : ' + str(roc_auc_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pred = []\n",
    "for j in range(len(x_test)):\n",
    "    sample = x_test[j]\n",
    "    min_dist = 9999\n",
    "    for i in range(len(x_train)):\n",
    "\n",
    "        temp = x_train[i]\n",
    "        #calculating norm\n",
    "        \n",
    "        dist = np.linalg.norm(sample - temp)\n",
    "        \n",
    "        if(dist<min_dist):\n",
    "            min_dist = dist\n",
    "            index = i\n",
    "    #appending the prediction\n",
    "    pred.append(index)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "y_pred =[]\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    if(y_train[pred[i]] == y_test[i]):\n",
    "        count = count + 1\n",
    "    y_pred.append(y_train[pred[i]])\n",
    "\n",
    "        \n",
    "acc = (float(count)/float(len(pred))) * 100       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 82.48337%\n",
      "Precision : 0.809603476266\n",
      "Execution time : 3.7094690799713135 seconds \n",
      "Recall : 0.824833702882\n",
      "F1_score : 0.814357077836\n",
      "Roc_Auc_score : 0.671659779614\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : {:.5f}%\".format(acc))\n",
    "print('Precision : ' + str(precision_score(y_test, y_pred, average= 'weighted')))\n",
    "print('Execution time : ' + str(end_time - start_time) + ' seconds ')\n",
    "print('Recall : ' + str(recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print('F1_score : ' + str(f1_score(y_test, y_pred, average = 'weighted')))\n",
    "print('Roc_Auc_score : ' + str(roc_auc_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "# sigmoid or logistic function we are gonna use \n",
    "def sigmoid(scores):\n",
    "    return ( 1.0 / (1.0 + np.exp(-scores)))\n",
    "\n",
    "\n",
    "\n",
    "# finding log likelihood : we derived this L(w) where w is a vector of parameters we wnt to learn for which prob.\n",
    "# of classlabel is maximized\n",
    "\n",
    "# in following weights are parameters we want to learn for which we get the best or optimum answer\n",
    "# features and labels (target vals) , scores is our hypothese function before applying sigmoid function\n",
    "def log_likelihood(features, target, weights): \n",
    "    scores = np.dot(features, weights)      # scores is (wTranspose x) , target is yi or labels \n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )      #look at this eqn we have derived as log likelihood\n",
    "    return ll\n",
    "\n",
    "# by taking derivative of loglikelihood eqn or ll we will get a gradient : which we ll use here\n",
    "\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1]).reshape(-1,1)     # intaliazing parameter values\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.array(np.dot(features, weights),dtype=np.float32 )    # h(x) = wT.x\n",
    "        predictions = sigmoid(scores)      # our hypotheses function after applying sigmoid  : h(x) = sigmoid(wT.x) \n",
    "        \n",
    "\n",
    "        # Update weights with gradient\n",
    "        #print(target)\n",
    "        #print(predictions)\n",
    "        output_error_signal = target - predictions      #  target means yi or actual labels , predictions means p(xi) or predicted prob.\n",
    "        gradient = np.dot(features.T, output_error_signal)    # this eqn we have derived : it is a derivative of ll : log likelihood\n",
    "        \n",
    "        weights += learning_rate * gradient       # forumula for G.D. Wnew = Wold + n * gradient : here + because we are maximizing\n",
    "        \n",
    "        # Print log-likelihood every so often\n",
    "        #if step % 10000 == 0:\n",
    "           # print(log_likelihood(features, target, weights))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#call to the function we have defined\n",
    "weights = logistic_regression(x_train, y_train,num_steps = 10000, learning_rate = 0.001, add_intercept=True)\n",
    "end =time.time()\n",
    "# these are the parameter values we have learned for optimum ( maximum ) confidence for given class label : frm training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy of logistic regression with gradient descent \n",
    "\n",
    "data_with_intercept = np.hstack((np.ones((x_test.shape[0], 1)),x_test))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "y_pred = np.round(sigmoid(final_scores))         # we are rounding final predicted prob. to neareset class label 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 84.7006651885 %\n",
      "Precision : 0.834707068131\n",
      "Execution time : 0.5269415378570557 seconds \n",
      "Recall : 0.847006651885\n",
      "F1_score : 0.822328750999\n",
      "Roc_Auc_score : 0.65099862259\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \" + str(100 * accuracy_score( y_pred, y_test ) ) + \" %\")\n",
    "print('Precision : ' + str(precision_score(y_test, y_pred, average= 'weighted')))\n",
    "print('Execution time : ' + str(end - start) + ' seconds ')\n",
    "print('Recall : ' + str(recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print('F1_score : ' + str(f1_score(y_test, y_pred, average = 'weighted')))\n",
    "print('Roc_Auc_score : ' + str(roc_auc_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic using Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sigmoid or logistic function we are gonna use \n",
    "def sigmoidN(scores):\n",
    "    return ( 1.0 / (1.0 + np.exp(-scores)))\n",
    "\n",
    "\n",
    "\n",
    "# finding log likelihood : we derived this L(w) where w is a vector of parameters we wnt to learn for which prob.\n",
    "# of classlabel is maximized\n",
    "\n",
    "# in following weights are parameters we want to learn for which we get the best or optimum answer\n",
    "# features and labels (target vals) , scores is our hypothese function before applying sigmoid function\n",
    "def log_likelihoodN(features, target, weights): \n",
    "    scores = np.dot(features, weights)      # scores is (wTranspose x) , target is yi or labels \n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )      #look at this eqn we have derived as log likelihood\n",
    "    return ll\n",
    "\n",
    "# by taking derivative of loglikelihood eqn or ll we will get a gradient : which we ll use here\n",
    "\n",
    "def logistic_regression_newton(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1]).reshape(-1,1)     # intaliazing parameter values\n",
    "    m = features.shape[0]\n",
    "    for step in range(num_steps):\n",
    "        scores = np.array(np.dot(features, weights),dtype = np.float64 )    # h(x) = wT.x\n",
    "        predictions = sigmoidN(scores)      # our hypotheses function after applying sigmoid  : h(x) = sigmoid(wT.x) \n",
    "   \n",
    "        sigma = np.dot(predictions, (1 - predictions).T)\n",
    "        sigma = np.diag((sigma)[:,0])\n",
    "        #print(np.shape(sigma))\n",
    "        temp = np.dot(sigma, features)\n",
    "        \n",
    "        #the hessian matrix\n",
    "        hessian = np.dot(features.T, temp)\n",
    "        \n",
    "        #inverse of hessian\n",
    "        hessian_inv = np.linalg.inv(hessian) / m     \n",
    "\n",
    "        # Update weights with gradient\n",
    "        #print(target)\n",
    "        #print(predictions)\n",
    "        output_error_signal = target - predictions      #  target means yi or actual labels , predictions means p(xi) or predicted prob.\n",
    "        gradient = np.dot(features.T, output_error_signal) / m  # this eqn we have derived : it is a derivative of ll : log likelihood\n",
    "        #print(np.shape(target))\n",
    "        weights += (np.dot(hessian_inv, gradient))       # forumula for G.D. Wnew = Wold + n * gradient : here + because we are maximizing\n",
    "        \n",
    "        # Print log-likelihood every so often\n",
    "        #if step % 10000 == 0:\n",
    "           # print(log_likelihood(features, target, weights))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#call to the function we have defined\n",
    "weights = logistic_regression_newton(x_train, y_train,num_steps = 5, learning_rate = 0.001, add_intercept=True)\n",
    "end =time.time()\n",
    "# these are the parameter values we have learned for optimum ( maximum ) confidence for given class label : frm training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of logistic regression with gradient descent \n",
    "\n",
    "data_with_intercept = np.hstack((np.ones((x_test.shape[0], 1)),x_test))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "y_pred = np.round(sigmoid(final_scores))         # we are rounding final predicted prob. to neareset class label 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 83.8137472284 %\n",
      "Precision : 0.82945364831\n",
      "Execution time : 0.1498725414276123 seconds \n",
      "Recall : 0.838137472284\n",
      "F1_score : 0.801481659146\n",
      "Roc_Auc_score : 0.611053719008\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \" + str(100 * accuracy_score( y_pred, y_test ) ) + \" %\")\n",
    "print('Precision : ' + str(precision_score(y_test, y_pred, average= 'weighted')))\n",
    "print('Execution time : ' + str(end - start) + ' seconds ')\n",
    "print('Recall : ' + str(recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print('F1_score : ' + str(f1_score(y_test, y_pred, average = 'weighted')))\n",
    "print('Roc_Auc_score : ' + str(roc_auc_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/Data_Q3/D21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.151624</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.132203</td>\n",
       "      <td>0.296852</td>\n",
       "      <td>-0.068419</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>0.357298</td>\n",
       "      <td>0.150411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.210474</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.018591</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.122228</td>\n",
       "      <td>0.090209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>-0.184451</td>\n",
       "      <td>-0.051672</td>\n",
       "      <td>0.009168</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.124602</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>0.046270</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         1         2         3         4         5         6  7\n",
       "0           0  0.046303  0.028466  0.151624  0.007990  0.005973  0.003635  1\n",
       "1           1  0.132203  0.296852 -0.068419  0.146222  0.357298  0.150411  0\n",
       "2           2 -0.210474  0.144329  0.018591  0.097309  0.122228  0.090209  1\n",
       "3           3  0.007505 -0.184451 -0.051672  0.009168  0.023768  0.011703  1\n",
       "4           4 -0.081010  0.124602 -0.033422  0.046270  0.032020  0.008429  1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046303</td>\n",
       "      <td>0.028466</td>\n",
       "      <td>0.151624</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.003635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.132203</td>\n",
       "      <td>0.296852</td>\n",
       "      <td>-0.068419</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>0.357298</td>\n",
       "      <td>0.150411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.210474</td>\n",
       "      <td>0.144329</td>\n",
       "      <td>0.018591</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.122228</td>\n",
       "      <td>0.090209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007505</td>\n",
       "      <td>-0.184451</td>\n",
       "      <td>-0.051672</td>\n",
       "      <td>0.009168</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.124602</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>0.046270</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6  7\n",
       "0  0.046303  0.028466  0.151624  0.007990  0.005973  0.003635  1\n",
       "1  0.132203  0.296852 -0.068419  0.146222  0.357298  0.150411  0\n",
       "2 -0.210474  0.144329  0.018591  0.097309  0.122228  0.090209  1\n",
       "3  0.007505 -0.184451 -0.051672  0.009168  0.023768  0.011703  1\n",
       "4 -0.081010  0.124602 -0.033422  0.046270  0.032020  0.008429  1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deleting the first column since it is use less numbering only\n",
    "df = df.drop('Unnamed: 0', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to find mean and std dev of all features for given class\n",
    "\n",
    "meanvector=[]\n",
    "features_count = df.shape[1]-1\n",
    "\n",
    "def findMeanVector(df):\n",
    "    '''find mean vector for features of given dataframe'''\n",
    "    for i in range(0,features_count):\n",
    "        mean = df.ix[:,i].mean()\n",
    "        meanvector.insert(i,mean)\n",
    "        \n",
    "    return meanvector\n",
    "\n",
    "#defining a function to find std dev of all features for given class\n",
    "\n",
    "stdvector=[]\n",
    "features_count = df.shape[1]-1\n",
    "\n",
    "def findStdVector(df):\n",
    "    '''find std dev vector for features of given dataframe'''\n",
    "    for i in range(0,features_count):\n",
    "        std = df.ix[:,i].std()\n",
    "        stdvector.insert(i,std)\n",
    "        \n",
    "    return stdvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gaussain pdf : calculateProbability() function\n",
    "\n",
    "import math\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    '''prob of feature given class label'''\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    "\n",
    "def probOfClass(datapt , df):   # means given data point values u find prob of tht point belonging this class\n",
    "    ''' will return prob of class label given features'''\n",
    "    pr = 1\n",
    "    meanVector = findMeanVector(df)\n",
    "    stdVector = findStdVector(df)\n",
    "    for i in range(0,6):    # 6 features\n",
    "        pr *= calculateProbability(datapt.iat[0,i],meanVector[i],stdVector[i])\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1         2         3         4         5         6  7\n",
      "629   0.009157  0.139985 -0.011941  0.023705  0.010136  0.011952  1\n",
      "995   0.393912  0.089227 -0.087177  0.063640  0.021430  0.116388  0\n",
      "2125 -0.222766  0.412373 -0.389669  0.013848  0.017464  0.014911  1\n",
      "1656 -0.222004  0.103720  0.005680  0.005797  0.004121  0.003718  1\n",
      "1593  0.262310  0.098346 -0.038579  0.066691  0.283348  0.033326  0\n"
     ]
    }
   ],
   "source": [
    "#features = irisDf.values[:,:4]    # till 3rd column means exclusive of 4th coln whch is target\n",
    "#target = irisDf.values[:,4]       # these are the target values or labels\n",
    "  \n",
    "#slicing data\n",
    "#features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "#                                                                   target, test_size = 0.33, random_state = 10)\n",
    "\n",
    "    \n",
    "# this will give dataframes as output of splitting \n",
    "traindf,testdf = train_test_split(df,test_size=0.33)\n",
    "print(traindf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#seperate data by class\n",
    "\n",
    "df0 = traindf.loc[traindf['7'] == 0]\n",
    "df1 = traindf.loc[traindf['7'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining a function to predict a class label\n",
    "\n",
    "def predictClassLabel(datapt):\n",
    "    '''predicts the class label for given data point'''\n",
    "    pr_0 = probOfClass(datapt, df0)\n",
    "    pr_1 = probOfClass(datapt, df1)\n",
    "    \n",
    "    m = max(pr_0, pr_1)\n",
    "    \n",
    "    if(m == pr_0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.31720430107528\n"
     ]
    }
   ],
   "source": [
    "#lets find accuracy : predict class labels for all test instances and then compare both of them\n",
    "\n",
    "import time\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "def findAccuracy(testdf):\n",
    "    '''find accurcay of an algorithm'''\n",
    "    count =0\n",
    "    total =0\n",
    "\n",
    "    for i in range(0,len(testdf)):\n",
    "        prediction = predictClassLabel(testdf.iloc[i:i+1,0:6])       # taking row by row without label \n",
    "\n",
    "        if(prediction == testdf.iloc[i]['7']):            # comparing predicted label with actual label\n",
    "            count += 1\n",
    "        \n",
    "        y_pred.append(prediction)\n",
    "        total += 1\n",
    "\n",
    "    return count/total\n",
    "\n",
    "\n",
    "print(100 * findAccuracy(testdf))\n",
    "\n",
    "\n",
    "# finding training time\n",
    "\n",
    "t0 = time.time()\n",
    "predictClassLabel(testdf.iloc[0:1,0:6])\n",
    "tm = time.time()-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.array(y_pred).reshape(-1,1)\n",
    "y_test = np.array(testdf.iloc[:,6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 81.3172043011 %\n",
      "Precision : 0.81170218457\n",
      "Execution time : 0.005013704299926758 seconds \n",
      "Recall : 0.813172043011\n",
      "F1_score : 0.812421690539\n",
      "Roc_Auc_score : 0.696805555556\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \" + str(100 * accuracy_score( y_pred, y_test ) ) + \" %\")\n",
    "print('Precision : ' + str(precision_score(y_test, y_pred, average= 'weighted')))\n",
    "print('Execution time : ' + str( tm ) + ' seconds ')\n",
    "print('Recall : ' + str(recall_score(y_test, y_pred, average = 'weighted')))\n",
    "print('F1_score : ' + str(f1_score(y_test, y_pred, average = 'weighted')))\n",
    "print('Roc_Auc_score : ' + str(roc_auc_score( y_test, y_pred )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
